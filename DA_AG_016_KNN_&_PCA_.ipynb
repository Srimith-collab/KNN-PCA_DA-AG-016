{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1** **What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?**\n",
        "\n",
        "K-Nearest Neighbors (KNN) is a supervised, non-parametric, instance-based machine learning algorithm.\n",
        "It does not learn an explicit model during training. Instead, it stores the training data and makes predictions by comparing new data points to the most similar existing ones.\n",
        "\n",
        "**Core Idea of KNN**\n",
        "\n",
        "“A data point is likely to be similar to the points closest to it.”\n",
        "\n",
        "KNN works by:\n",
        "\n",
        "1. Measuring the distance between a new data point and all training points\n",
        "\n",
        "2. Selecting the K closest points (neighbors)\n",
        "\n",
        "3. Making a prediction based on those neighbors\n",
        "\n",
        "Common distance metrics:\n",
        "\n",
        "*   Euclidean distance (most common)\n",
        "*   Manhattan distance\n",
        "*   Minkowski distance\n",
        "*   Cosine similarity (for text/high-dimensional data)\n",
        "\n",
        "**How KNN Works – Step by Step**\n",
        "\n",
        "* Choose a value for K (number of neighbors)\n",
        "* Calculate the distance between the new point and all training points\n",
        "* Sort distances in ascending order\n",
        "* Select the K nearest neighbors\n",
        "* Predict output based on those neighbors\n",
        "\n",
        "**KNN for Classification**\n",
        "\n",
        "Used when the target variable is categorical (e.g., Yes/No, Class A/B/C).\n",
        "\n",
        "**Prediction Rule:**\n",
        "\n",
        "* The new data point is assigned the most frequent class among its K nearest neighbors\n",
        "* This is called majority voting\n",
        "\n",
        "This can be understood by an example:\n",
        "\n",
        "If K = 5 and nearest neighbors are:\n",
        "\n",
        "* Class A → 3 points\n",
        "* Class B → 2 points\n",
        "\n",
        "**Predicted class = Class A**\n",
        "\n",
        "**Weighted KNN (optional):**\n",
        "\n",
        "Closer neighbors are given higher weight, reducing the impact of distant points.\n",
        "\n",
        "Used when the target variable is continuous (e.g., price, demand, temperature).\n",
        "\n",
        "**Prediction Rule:**\n",
        "\n",
        "* The predicted value is the average (or weighted average) of the K nearest neighbors’ values\n",
        "\n",
        "**Example:**\n",
        "\n",
        "If K = 4 and neighbor values are:\n",
        "\n",
        "* 10, 12, 14, 16\n",
        "\n",
        "Predicted value = (10 + 12 + 14 + 16) / 4 = 13\n",
        "\n",
        "Weighted regression gives higher importance to closer neighbors.\n",
        "\n",
        "**Choosing the Value of K**\n",
        "\n",
        "* Small K (e.g., 1–3):\n",
        "\n",
        "    * Sensitive to noise\n",
        "\n",
        "    * Risk of overfitting\n",
        "\n",
        "* Large K\n",
        "\n",
        "    * Smoother decision boundary\n",
        "\n",
        "    * Risk of underfitting\n",
        "\n",
        "Best K is usually found using cross-validation\n",
        "\n",
        "**Advantages of KNN**\n",
        "\n",
        "*   Simple and intuitive\n",
        "*   No training phase (lazy learning)\n",
        "*   Works well with small datasets\n",
        "*   Can model complex decision boundaries\n",
        "\n",
        "**Disadvantages of KNN**\n",
        "\n",
        "* Computationally expensive at prediction time\n",
        "* Requires feature scaling\n",
        "* Sensitive to irrelevant features and noise\n",
        "* Poor performance on very large datasets\n",
        "\n",
        "**When to Use KNN**\n",
        "\n",
        "* When dataset size is small to medium\n",
        "* When relationships are non-linear\n",
        "* As a baseline model for comparison\n",
        "* For recommendation systems and pattern recognition\n",
        "\n",
        "| Aspect     | Classification            | Regression                    |\n",
        "| ---------- | ------------------------- | ----------------------------- |\n",
        "| Output     | Class label               | Continuous value              |\n",
        "| Decision   | Majority vote             | Mean / weighted mean          |\n",
        "| Common Use | Spam detection, diagnosis | Price prediction, forecasting |\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BhdJWAd4wFJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?**\n",
        "\n",
        "The Curse of Dimensionality refers to a set of problems that arise when working with high-dimensional data (i.e., data with many features). As the number of dimensions increases, data becomes sparse, distances lose meaning, and many machine learning algorithms—especially K-Nearest Neighbors (KNN)—perform poorly.\n",
        "\n",
        "**What is the Curse of Dimensionality?**\n",
        "\n",
        "In simple terms: **As dimensions increase, the volume of the feature space grows exponentially, but the amount of data does not.**\n",
        "\n",
        "This causes data points to be far apart and less comparable, even if the dataset is large.\n",
        "\n",
        "**Intuition**\n",
        "\n",
        "* In 1D, points lie on a line\n",
        "* In 2D, points lie on a plane\n",
        "* In 100D, points are scattered in a massive space with huge empty regions\n",
        "\n",
        "Even millions of data points cannot adequately cover a high-dimensional space.\n",
        "\n",
        "**Why KNN is Strongly Affected?**\n",
        "\n",
        "KNN relies entirely on distance calculations to find “nearest” neighbors. In high dimensions, distance measures break down.\n",
        "\n",
        "**Key Effects on KNN Performance**\n",
        "\n",
        "1. Distance Concentration Problem\n",
        "\n",
        "As dimensions increase:\n",
        "\n",
        "* The distance to the nearest neighbor becomes almost the same as the distance to the farthest neighbor\n",
        "\n",
        "* The ratio between nearest and farthest distances → 1\n",
        "\n",
        "All points appear equally far away as a results KNN cannot reliably identify “nearest” neighbors.\n",
        "\n",
        "2. Sparsity of Data\n",
        "\n",
        "* High-dimensional space grows exponentially\n",
        "* Data points occupy only a tiny fraction of the space\n",
        "\n",
        "Neighbors are not truly similar, even if they are “closest” because of which predictions become noisy and unreliable.\n",
        "\n",
        "3. Increased Noise and Irrelevant Features\n",
        "\n",
        "* Many features may be irrelevant or weakly informative\n",
        "* Each extra dimension adds noise to distance calculations\n",
        "\n",
        "Irrelevant features dominate distance metrics due to this KNN accuracy drops significantly.\n",
        "\n",
        "4. Exponential Increase in Computation\n",
        "\n",
        "* Distance must be computed across all dimensions\n",
        "* Higher dimensions → higher computational cost\n",
        "\n",
        "As a result, KNN becomes slow and inefficient\n",
        "\n",
        "**Mathematical Insight (Simplified)**\n",
        "\n",
        "For a unit hypercube:\n",
        "\n",
        "* To cover just 1% of the volume:\n",
        "\n",
        "  * 1D → interval length = 0.01\n",
        "  * 10D → length ≈ 0.63\n",
        "  * 100D → length ≈ 0.99\n",
        "\n",
        "Almost the entire range is needed to find neighbors.\n",
        "\n",
        "| Effect              | Impact on KNN               |\n",
        "| ------------------- | --------------------------- |\n",
        "| Distance similarity | Neighbors indistinguishable |\n",
        "| Sparsity            | Poor generalization         |\n",
        "| Noise accumulation  | Lower accuracy              |\n",
        "| Computation         | Slower predictions          |\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "60NlJyWK6Jcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?**\n",
        "\n",
        "**What is Principal Component Analysis (PCA)?**\n",
        "\n",
        "Principal Component Analysis (PCA) is an unsupervised dimensionality reduction technique used to transform a high-dimensional dataset into a smaller set of new variables, called principal components, while retaining as much variance (information) as possible.\n",
        "\n",
        "Instead of selecting existing features, PCA creates new features.\n",
        "\n",
        "**Key Idea of PCA**\n",
        "\n",
        "Find new axes (directions) in the data that capture maximum variance and are uncorrelated.\n",
        "\n",
        "These new axes:\n",
        "\n",
        "*   Are called principal components (PCs)\n",
        "*   Are linear combinations of the original features\n",
        "*   Are ordered by importance (variance explained)\n",
        "\n",
        "**How PCA Works (Step-by-Step)**\n",
        "\n",
        "1. Standardize the data\n",
        "\n",
        "(Mean = 0, Variance = 1)\n",
        "\n",
        "2. Compute the covariance matrix\n",
        "\n",
        "3. Calculate eigenvalues and eigenvectors\n",
        "\n",
        "      * Eigenvectors → directions (principal components)\n",
        "\n",
        "      * Eigenvalues → amount of variance captured\n",
        "\n",
        "4. Sort components by decreasing eigenvalues\n",
        "\n",
        "5. Select top k components\n",
        "\n",
        " (based on explained variance, e.g., 95%)\n",
        "\n",
        "6. Project data onto selected components\n",
        "\n",
        "*Example (Intuition)*\n",
        "\n",
        "Suppose you have:\n",
        "\n",
        "* Height\n",
        "* Weight\n",
        "\n",
        "These are correlated. PCA may create:\n",
        "\n",
        "* PC1 → overall body size (captures most variance)\n",
        "* PC2 → body proportion (captures less variance)\n",
        "\n",
        "**What is Feature Selection?**\n",
        "\n",
        "Feature Selection is the process of choosing a subset of the original features that are most relevant to the target variable.\n",
        "\n",
        "* No new features are created\n",
        "* Original features remain interpretable\n",
        "\n",
        "**Types of Feature Selection**\n",
        "\n",
        "1. Filter Methods\n",
        "\n",
        "   * Correlation\n",
        "   * Chi-square\n",
        "   * Mutual information\n",
        "\n",
        "2. Wrapper Methods\n",
        "\n",
        "   * Forward selection\n",
        "   * Backward elimination\n",
        "   * Recursive Feature Elimination (RFE)\n",
        "\n",
        "3. Embedded Methods\n",
        "\n",
        "  * LASSO (L1 regularization)\n",
        "  * Decision tree feature importance\n",
        "\n",
        "**PCA vs Feature Selection (Key Differences)**\n",
        "\n",
        "| Aspect                     | PCA                            | Feature Selection           |\n",
        "| -------------------------- | ------------------------------ | --------------------------- |\n",
        "| Approach                   | Feature **transformation**     | Feature **selection**       |\n",
        "| Output                     | New components                 | Subset of original features |\n",
        "| Interpretability           | Low                            | High                        |\n",
        "| Uses target variable       | ❌ No (unsupervised)            | ✅ Yes (usually)             |\n",
        "| Handles multicollinearity  | Excellent                      | Limited                     |\n",
        "| Preserves original meaning | ❌ No                           | ✅ Yes                       |\n",
        "| Best for                   | Noise reduction, visualization | Model interpretability      |\n",
        "\n"
      ],
      "metadata": {
        "id": "sdBZwGsB8a80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 4: What are eigenvalues and eigenvectors in PCA, and why are they important?**\n",
        "\n",
        "**Eigenvalues and Eigenvectors in PCA (Principal Component Analysis)**\n",
        "\n",
        "In PCA, eigenvalues and eigenvectors come from the covariance matrix (or correlation matrix) of the data. They are the mathematical foundation that tells us which directions matter most in the data.\n",
        "\n",
        "1. Eigenvectors in PCA – Directions of Maximum Variance\n",
        "\n",
        "An eigenvector represents a direction (axis) in the feature space along which the data varies.\n",
        "\n",
        "In PCA:\n",
        "\n",
        "* Each eigenvector = a principal component\n",
        "* It shows how original features are combined\n",
        "* Eigenvectors are orthogonal (uncorrelated) to each other\n",
        "\n",
        "*Intuition*\n",
        "\n",
        "Eigenvectors tell us “which way to look” at the data to see the most structure.\n",
        "\n",
        "**Example**\n",
        "\n",
        "For two features X1 and X2:\n",
        "\n",
        "An eigenvector might look like:\n",
        "\n",
        "             PC1 = 0.7X1 + 0.7X2\n",
        "\n",
        "This means PC1 is a linear combination of the original features.\n",
        "\n",
        "2. Eigenvalues in PCA – Amount of Information\n",
        "\n",
        "An eigenvalue tells us how much variance (information) is captured along its corresponding eigenvector.\n",
        "\n",
        "In PCA:\n",
        "\n",
        "* Larger eigenvalue → more variance explained\n",
        "* Smaller eigenvalue → less useful information\n",
        "\n",
        "*Intuition*\n",
        "\n",
        "Eigenvalues tell us “how important” each direction is.\n",
        "\n",
        "3. Relationship Between Eigenvalues and Eigenvectors\n",
        "\n",
        "They always come in pairs:\n",
        "\n",
        "| Eigenvector         | Eigenvalue            |\n",
        "| ------------------- | --------------------- |\n",
        "| Direction           | Importance            |\n",
        "| Axis                | Variance              |\n",
        "| Principal component | Explained information |\n",
        "\n",
        "4. How PCA Uses Eigenvalues and Eigenvectors\n",
        "\n",
        "Step-by-step role in PCA:\n",
        "\n",
        "  * Standardize data\n",
        "  * Compute covariance matrix\n",
        "  * Find eigenvalues and eigenvectors\n",
        "  * Sort eigenvectors by eigenvalues (descending)\n",
        "  * Select top k eigenvectors\n",
        "  * Project data onto them\n",
        "\n",
        "This creates a lower-dimensional representation with minimal information loss.\n",
        "\n",
        "5. Explained Variance Ratio:\n",
        "\n",
        "The explained variance ratio is calculated as:\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "This helps decide:\n",
        "\n",
        "* How many components to keep\n",
        "* When 90–95% of variance is retained\n",
        "\n",
        "6. Why Eigenvalues and Eigenvectors Are Important in PCA\n",
        "\n",
        "* Dimensionality Reduction\n",
        "    * They identify the most informative directions\n",
        "    * Allow removal of low-variance (noise) components\n",
        "\n",
        "* Noise Reduction\n",
        "\n",
        "    * Small eigenvalues often represent noise\n",
        "    * Removing them improves model performance\n",
        "\n",
        "* Feature Decorrelation\n",
        "\n",
        "   * Eigenvectors are orthogonal\n",
        "   * Removes multicollinearity\n",
        "\n",
        "* Model Performance Improvement\n",
        "\n",
        "  * Helps distance-based models (KNN, SVM)\n",
        "  * Reduces overfitting\n"
      ],
      "metadata": {
        "id": "mIbbkYOQJDOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 5: How do KNN and PCA complement each other when applied in a single pipeline?**\n",
        "\n",
        "KNN and PCA complement each other extremely well when used in a single machine-learning pipeline because PCA fixes KNN’s main weaknesses, especially those caused by high dimensionality.\n",
        "\n",
        "**Why KNN Needs PCA**\n",
        "\n",
        "KNN is:\n",
        "\n",
        "* Distance-based\n",
        "* Sensitive to noise\n",
        "* Strongly affected by the curse of dimensionality\n",
        "\n",
        "As the number of features increases:\n",
        "\n",
        "* Distances become meaningless\n",
        "* Nearest neighbors are no longer “near”\n",
        "* Accuracy drops and computation slows\n",
        "\n",
        "PCA solves these problems before KNN is applied.\n",
        "\n",
        "**How PCA Helps KNN (Conceptual View)**\n",
        "\n",
        "PCA:\n",
        "\n",
        "* Removes irrelevant and redundant dimensions\n",
        "* Compresses information into fewer, uncorrelated components\n",
        "* Preserves maximum variance\n",
        "\n",
        "KNN:\n",
        "\n",
        "* Computes distances more reliably\n",
        "* Finds truly similar neighbors\n",
        "* Runs faster\n",
        "\n",
        "**KNN + PCA Pipeline (Step-by-Step)**\n",
        "\n",
        "1. Feature Scaling (Mandatory)\n",
        "\n",
        "KNN and PCA both depend on distance.\n",
        "* StandardScaler / Min-Max Scaler\n",
        "\n",
        "2. Apply PCA\n",
        "\n",
        "* Reduce dimensionality\n",
        "* Remove multicollinearity\n",
        "* Retain 90–95% variance\n",
        "\n",
        "3. Apply KNN\n",
        "\n",
        "* Compute distances in PCA space\n",
        "* Perform classification or regression\n",
        "\n",
        "**What Each Algorithm Contributes**\n",
        "\n",
        "| PCA Contribution       | KNN Benefit                  |\n",
        "| ---------------------- | ---------------------------- |\n",
        "| Reduces dimensions     | Faster distance computation  |\n",
        "| Removes noise          | Higher accuracy              |\n",
        "| Eliminates correlation | Meaningful neighbor search   |\n",
        "| Handles sparsity       | Less curse of dimensionality |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TcbGkdXmPoxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ja5e-8stRZJZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WMZh-L3jv4zs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca8a27f2-6be4-4744-9df4-95e260fe48b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7777777777777778\n",
            "Accuracy with scaling: 0.9333333333333333\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# KNN WITHOUT feature scaling\n",
        "# -----------------------------\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "\n",
        "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# -----------------------------\n",
        "# KNN WITH feature scaling\n",
        "# -----------------------------\n",
        "knn_with_scaling = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\", KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "\n",
        "knn_with_scaling.fit(X_train, y_train)\n",
        "y_pred_scaling = knn_with_scaling.predict(X_test)\n",
        "accuracy_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "\n",
        "print(\"Accuracy without scaling:\", accuracy_no_scaling)\n",
        "print(\"Accuracy with scaling:\", accuracy_scaling)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.**\n"
      ],
      "metadata": {
        "id": "WNC4ouPAUTnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the Wine dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Standardize features (important for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Fit PCA (keep all components)\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Print explained variance ratio\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_, start=1):\n",
        "    print(f\"Principal Component {i}: {ratio:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6qmUw2IUeBF",
        "outputId": "4526f320-0d2c-4168-d055-11fb0d5aec49"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Principal Component 1: 0.3620\n",
            "Principal Component 2: 0.1921\n",
            "Principal Component 3: 0.1112\n",
            "Principal Component 4: 0.0707\n",
            "Principal Component 5: 0.0656\n",
            "Principal Component 6: 0.0494\n",
            "Principal Component 7: 0.0424\n",
            "Principal Component 8: 0.0268\n",
            "Principal Component 9: 0.0222\n",
            "Principal Component 10: 0.0193\n",
            "Principal Component 11: 0.0174\n",
            "Principal Component 12: 0.0130\n",
            "Principal Component 13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.**\n",
        "\n"
      ],
      "metadata": {
        "id": "L9LrZt8WUn8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# KNN on ORIGINAL data (with scaling)\n",
        "# -------------------------------------------------\n",
        "knn_original = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\", KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "\n",
        "knn_original.fit(X_train, y_train)\n",
        "y_pred_original = knn_original.predict(X_test)\n",
        "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# KNN on PCA-reduced data (top 2 components)\n",
        "# -------------------------------------------------\n",
        "knn_pca = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"pca\", PCA(n_components=2)),\n",
        "    (\"knn\", KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "\n",
        "knn_pca.fit(X_train, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "print(\"Accuracy on original dataset:\", accuracy_original)\n",
        "print(\"Accuracy on PCA (2 components):\", accuracy_pca)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8tJBXhXU3pm",
        "outputId": "138f59c9-2345-4f35-c51b-8744a2f5e6f4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on original dataset: 0.9333333333333333\n",
            "Accuracy on PCA (2 components): 0.9333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 9: Train a KNN Classifier with different distance metrics (euclidean,manhattan) on the scaled Wine dataset and compare the results.**\n"
      ],
      "metadata": {
        "id": "gfB1keEpVAT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# KNN with Euclidean distance\n",
        "# -----------------------------\n",
        "knn_euclidean = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\", KNeighborsClassifier(\n",
        "        n_neighbors=5,\n",
        "        metric=\"euclidean\"\n",
        "    ))\n",
        "])\n",
        "\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# -----------------------------\n",
        "# KNN with Manhattan distance\n",
        "# -----------------------------\n",
        "knn_manhattan = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\", KNeighborsClassifier(\n",
        "        n_neighbors=5,\n",
        "        metric=\"manhattan\"\n",
        "    ))\n",
        "])\n",
        "\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "print(\"Accuracy (Euclidean):\", accuracy_euclidean)\n",
        "print(\"Accuracy (Manhattan):\", accuracy_manhattan)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vObgBuDVSJW",
        "outputId": "ba7bc80f-dc31-4069-c64d-b962d59f1596"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Euclidean): 0.9333333333333333\n",
            "Accuracy (Manhattan): 0.9777777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10: You are working with a high-dimensional gene expression dataset toclassify patients with different types of cancer.Due to the large number of features and a small number of samples, traditional models overfit.\n",
        "\n",
        "Explain how you would:\n",
        "* Use PCA to reduce dimensionality\n",
        "* Decide how many components to keep\n",
        "* Use KNN for classification post-dimensionality reduction\n",
        "* Evaluate the model\n",
        "* Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data\n"
      ],
      "metadata": {
        "id": "B62ubbLXVWsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Simulate high-dimensional gene expression data\n",
        "X, y = make_classification(\n",
        "    n_samples=120,\n",
        "    n_features=5000,\n",
        "    n_informative=50,\n",
        "    n_classes=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 1. Linear SVM on raw data (overfitting)\n",
        "svm_raw = SVC(kernel=\"linear\")\n",
        "svm_raw.fit(X_train, y_train)\n",
        "\n",
        "train_acc_raw = accuracy_score(y_train, svm_raw.predict(X_train))\n",
        "test_acc_raw = accuracy_score(y_test, svm_raw.predict(X_test))\n",
        "\n",
        "# 2. PCA + Linear SVM\n",
        "pca = PCA(n_components=50)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "svm_pca = SVC(kernel=\"linear\")\n",
        "svm_pca.fit(X_train_pca, y_train)\n",
        "\n",
        "train_acc_pca = accuracy_score(y_train, svm_pca.predict(X_train_pca))\n",
        "test_acc_pca = accuracy_score(y_test, svm_pca.predict(X_test_pca))\n",
        "\n",
        "print(\"Raw SVM - Train Accuracy:\", train_acc_raw)\n",
        "print(\"Raw SVM - Test Accuracy:\", test_acc_raw)\n",
        "print(\"PCA + SVM - Train Accuracy:\", train_acc_pca)\n",
        "print(\"PCA + SVM - Test Accuracy:\", test_acc_pca)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqXmvEizVv24",
        "outputId": "7567e5fe-621f-4413-b4be-11e19e80071e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw SVM - Train Accuracy: 1.0\n",
            "Raw SVM - Test Accuracy: 0.5\n",
            "PCA + SVM - Train Accuracy: 1.0\n",
            "PCA + SVM - Test Accuracy: 0.5555555555555556\n"
          ]
        }
      ]
    }
  ]
}